[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anotações de Inferência Bayesiana",
    "section": "",
    "text": "1 Inferência Bayesiana",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inferência Bayesiana</span>"
    ]
  },
  {
    "objectID": "pensamento-bayesiano.html",
    "href": "pensamento-bayesiano.html",
    "title": "2  O Pensamento Bayesiano",
    "section": "",
    "text": "2.1 Inferência\nPodemos dizer que inferência é qualquer processo racional de redução de incerteza. Se não há incerteza, não há necessidade da estatística.\nNote que, mesmo após esse processo, a incerteza sobre a composição da caixa contínua. Contudo, com a nova informação, ocorre uma atualização da incerteza.\nA inferência estatística é, portanto, o processo de redução de incertezas a partir de dados e métodos estatísticos. Neste contexto, a Inferência Bayesiana é a inferência estatística baseada na perspectiva subjetiva de probabilidade.",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O Pensamento Bayesiano</span>"
    ]
  },
  {
    "objectID": "pensamento-bayesiano.html#inferência",
    "href": "pensamento-bayesiano.html#inferência",
    "title": "2  O Pensamento Bayesiano",
    "section": "",
    "text": "Exemplo 2.1 (Bolas na caixa) Considere uma caixa com 5 bolas, algumas verdes (\\(v\\)), e o restante brancas (\\(b\\)). Suponha que amostramos duas bolas, sem reposição. Isto é, retiramos duas bolas desta caixa, e que ambas as bolas retiradas foram verdes. A partir deste experimento, podemos inferir que \\(v \\geq 2\\), ou seja, ao menos duas bolas da caixa são verdes. Caso retirássemos uma branca e uma verde, poderíamos naturalmente inferir que ao menos uma bola da caixa é branca, e ao menos uma é verde.\n\n\n\n\n\n\n\n\n\nProbabilidade\n\n\n\nO objeto probabilidade pode ser estudada de diversas perspectivas. Estamos acostumados com o cálculo de probabilidade, em que desenvolvemos teoremas e outros resultados a partir dos axiomas de Kolmogorov.\n\\[\n\\begin{aligned}\nP(A) \\geq 0, \\forall A \\in \\mathcal{F} \\\\\nP(\\Omega) = 1 \\\\\nP\\left(\\bigcup\\limits_{n=1}^\\infty A_n\\right) = \\sum^\\infty_{n=1} A_n\n\\end{aligned}\n\\]\nPor outro lado, há pessoas que se ocupam em entender a probabilidade do ponto de vista da Teoria da Medida, preocupando-se em estudar a probabilidade como uma função de medida.\nAinda assim, existe uma visão mais “filosófica” do estudo da probabilidade, que se preocupa na interpretação da probabilidade. Uma interpretação clássica é da probabilidade como medida de frequência, enquanto a interpretação subjetiva da probabilidade, utilizada na inferência Bayesiana, toma a probabilidade como uma medida de incerteza.\nPor exemplo, num lançamento de moedas, a teoria frequentista afirma que, ao lançar uma moeda honesta um número grande de vezes, esperaria que 50% dos lançamentos resultasse em cara. Por outro lado, na teoria subjetiva da probabilidade, alguém que acredita que a moeda é honesta diria, antes do primeiro lançamento desta, que não há razão de acreditar que a moeda vá provavelmente cair cara, tão quanto acreditar que mais provavelmente cairá coroa. Nesta visão, a probabilidade é um número que mede, quantifica ou representa a incerteza do observador sobre um fenômeno.\nEsta discussão sobre o significado de probabilidade é denominado Teoria da Probabilidade.\n\n\n\n2.1.1 Cenários de Incerteza\nAlém do exemplo da caixa com bolas que discutimos anteriormente, podemos pensar em outros cenários de incerteza:\n\nExemplo 2.2 (Erupções na pele) Considere um indivíduo com erupções na pele. Este tipo de erupções pode ser causado por doenças diversas, dentre elas, uma é considerada grave, provocando incerteza no indivíduo sobre sua saúde. Sabendo disto, o indivíduo vai ao hospital em busca de informações sobre sua condição. A partir de experimentos, como exames médicos, a incerteza do indivíduo sobre a doença e sua saúde é reduzida pelas informações obtidas.\n\n\nExemplo 2.3 (Avião perdido) Em outro exemplo, considere que um avião caiu num corpo d’água e sua localização exata é desconhecida. Conforme descobrimos informações do avião, como partes encontradas no mar, consideração de oceanográficos e climáticos, análise técnica do modelo do avião, resultados de buscas passadas e outras informações, podemos gradativamente reduzir a região de queda do avião, até encontrarmos o ponto exato ou muito próximo desta queda.\nEste exemplo é mais real do que pensa! Técnicas bayesianas foram utilizadas na busca pelo infame voo 447\n\nNum último exemplo, suponha que estamos interessados em estudar a eficácia de um medicamento na redução da taxa do colesterol. A incerteza jaz exatamente na eficácia do medicamento, isto é, quanto a taxa de colesterol é reduziada após uso do medicamento. Para esta análise, coletaríamos amostras aleatórias simples. Suponha, por hipótese, que o colesterol dos pacientes em estudo amostrados (\\(\\boldsymbol{X}_n = (X_1, \\dots, X_n)\\) seguem uma distribuição Normal, com variância 16 e média desconhecida que queremos inferir:\n\\[\nX \\sim (?,16)\n\\]\nNo modelo clássico, usaríamos de métodos como o uso do estimador não viesado de variância uniformemente mínima. Por outro lado, o bayesiano começaria com uma incerteza antes da coleta da amostra e, com a informação obtida, atualizaria sua incerteza em uma nova distribuição: \\[\n\\mathcal{D} \\stackrel{\\boldsymbol{X}_n}{\\rightarrow} \\mathcal{D} \\lvert x_1,\n\\dots, x_n\n\\]\nEsta ferramenta de atualização de informação é o cerne da abordagem Bayesiana, e será extensivamente estudada pelo curso.",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O Pensamento Bayesiano</span>"
    ]
  },
  {
    "objectID": "teorema-de-bayes.html",
    "href": "teorema-de-bayes.html",
    "title": "3  Teorema de Bayes",
    "section": "",
    "text": "Da probabilidade, conhecemos o enunciado do Teorema de Bayes.\n\nTeorema 3.1 (Teorema de Bayes) Sejam \\(A_1, A_2, ... A_n \\in \\mathcal{A}\\) eventos que particionam \\(\\Omega\\), isto é, \\(A_i \\cap A_j = \\emptyset, \\bigcup_{i=1}^n A_i = \\Omega,\\) e \\(D \\in\n\\mathcal{A}\\) um evento tal que \\(P(D) &gt; 0\\). Então, para \\(i = 1, 2, \\dots, n\\),\n\\[\nP(A_i \\rvert D) = \\frac{P(D \\rvert A_i) P(A_i)}{\\sum\\limits^{n}_{j = 1} P(D \\rvert\nA_j)P(A_j)}\n\\]\n\n\n\n\n\n\n\nDetalhes importantes\n\n\n\nNote que \\(P(X=x \\rvert \\theta = \\theta_i)\\) é a função de verossimilhança. Note também que o denominador, \\(\\sum\\limits^{n}_{j = 1} P(D \\rvert A_j)P(A_j)\\), não depende de \\(i\\)!!!\n\n\n\nNote que, para o Exemplo 2.1,  \n\\[\n\\begin{aligned}\nE(\\theta) &= \\frac{5}{2} \\\\\nE(\\theta \\rvert X_1 = 1, X_2 = 0)  &= 1 \\cdot \\frac{2}{10} + 2 \\cdot {3}{10} + 3\n\\cdot \\frac {3}{10} + 4 \\cdot \\frac{2}{10} = \\frac{5}{2} \\\\\n\\mathrm{Var}(\\theta \\rvert X_1 = 1, X_2 = 0) &= E(\\theta^2 \\rvert X_1 = 1, X_2=  0) - E(\\theta \\rvert X_1 = 1, X_2\n= 0)^2 \\\\\n&= 1^2 \\cdot \\frac{2}{10} + 2^2 \\cdot \\frac{3}{10} + 3^2\n\\cdot \\frac{3}{10} + 4^2 \\cdot \\frac{2}{10} - \\frac{25}{4} = \\frac{21}{20}\n\\end{aligned}\n\\]\n\nExercício 3.1 Refaça o exercício (encontre a distribuição a posteriori de \\(\\theta\\)) dado a amostra \\(X_1 = 1, X_2 = 1\\)\n\n\nSolução 3.1. Pelo Teorema 3.1,\n\\[\n\\frac{\\frac{i}{5} \\cdot \\frac{i-1}{4} \\cdot \\frac{1}{6}}{\\sum\\limits^5_{j=0}\n\\frac{j}{5} \\cdot \\frac{j-1}{4} \\cdot \\frac{1}{6}} = \\frac{i(i-1)}\n{\\sum\\limits^5_{j=0}j(j-1)} = \\frac{i(i-1)}{40}\n\\] \nUm outro caminho para resolução envolve o uso de proporcionalidade. Podemos observar que, pelo Teorema 3.1, \\[\n\\begin{aligned}\nP(\\theta = i \\rvert \\boldsymbol{X} = (1,1)) &\\propto\nP(\\boldsymbol{X} = (1,1) \\rvert \\theta = i)P(\\theta = i) = \\frac{i}{5} \\cdot\n\\frac{i-1}{4} \\cdot \\frac{1}{6} \\mathbb{1}_{\\Theta}(i) \\\\\n&= \\frac{i(i-1)}{120}\n\\mathbb{1}_{\\Theta}(i) \\propto i(i-1) \\mathbb{1}_{\\Theta}(i)\n\\end{aligned}\n\\]\nou seja, podemos simplesmente nos preocupar com o numerador e dividir (“normalizar”) pela soma dos termos: \n\n\nExemplo 3.1 (Informação sobre a soma) Ainda no cenário do Exemplo 2.1, considere a função \\(T(\\boldsymbol{X}) = X_1 +\nX_2\\) do número de bolas verdes na amostra. Você é informado que \\(T(X_ = 1\\). Vamos determinar a distribuição a posteriori de \\(\\theta\\) dado que \\(X_1 + X_2 = 1\\).\n\\[\nP(\\theta = i \\rvert X_1 + X_2 = 1) = \\frac{P(X_1 + X_2 = 1 \\rvert \\theta =\ni)P(\\theta=i)}{\\sum\\limits^5_{j=0}P(X_1 + X_2 = 1 \\rvert \\theta = j)P(\\theta=j)}\n\\] usando da proporcionalidade, \\[\n\\begin{aligned}\nP(\\theta = i \\rvert X_1 + X_2 = 1) &\\propto P(X_1 + X_2 \\rvert \\theta =\ni)P(\\theta = i)\\\\\n&= \\sum\\limits_{(x_1,x_2) \\in \\mathfrak{X} : x_1 + x_2 = 1}\nP(X_1 = x_1, X_2 = x_2 \\rvert \\theta = i)P(\\theta = i) \\\\\n&= [P(X_1 = 1, X_2 = 0 \\rvert \\theta = i) + P(X_1 = 0, X_2 = 1 \\rvert \\theta = i)]P(\\theta = i) \\\\\n&= \\left[\\frac{i}{5} \\cdot \\frac{(5-i)}{4} + \\frac{5-i}{5} \\cdot \\frac{i}{4}\\right] \\cdot \\frac{1}{6} \\mathbb{1}_\\Theta(i) \\\\\n&= \\frac{i(5-i)}{10} \\cdot \\frac{1}{6} \\mathbb{1}_\\Theta(i) \\propto i(5-1)\n\\mathbb{1}_\\Theta(i)\n\\end{aligned}\n\\]\nNote que isso nos concede a mesma quantidade de informação para a inferência que a amostra completa, ou seja, obtemos a mesma a posteriori utilizando esta estatística. Isto ocorre uma vez que a função \\(T(\\boldsymbol{X})\\) é uma estatística suficiente no sentido bayesiano. Mais adiante, discutiremos isso com mais detalhes.\n\nNo Exemplo 2.3 (do avião perdido), temos que \\(\\Theta = \\{1,2,3\\}\\) e \\[\nX = \\begin{cases}\n1, & \\text{Busca bem sucedida na região 1}, \\\\\n0, & \\text{Caso contrário},\n\\end{cases}\n\\] com \\(X \\rvert \\theta = 1 \\sim \\mathrm{Ber}\\left(\\frac{9}{10}\\right)\\) (ou seja, a busca na região tem probabilidade de encontrar o avião, se lá ele estiver, de 90%. Por análise de especialistas, temos as seguintes crenças sobre as regiões: \\[\n\\begin{aligned}\nP(\\theta = 1) &= \\frac{5}{10} \\\\\nP(\\theta = 2) &= \\frac{3}{10} \\\\\nP(\\theta = 3) &= \\frac{2}{10}.\n\\end{aligned}\n\\]\nSuponhamos \\(X = 0\\), isto é, investigamos a primeira região e não obtivemos êxito em encontrar o avião.\n\\[\n\\begin{aligned}\nP(\\theta = 1 \\rvert X = 0) &= \\frac{P(X = 0 \\rvert \\theta = 1)P(\\theta = 1)}\n{\\sum\\limits^3_{j=1}P(X = 0 \\rvert \\theta = j)P(\\theta = j)} \\\\\n&= \\frac{\\frac{1}{10} \\cdot \\frac{5}{10}}{\\frac{1}{10} \\cdot \\frac{5}{10} + 1\n\\cdot \\frac{3}{10} + 1 \\cdot \\frac{2}{10}} \\\\\n&= \\frac{5}{5 + 30 + 20} = \\frac{1}{11}.\n\\end{aligned}\n\\]\nDo mesmo modo, \\[\n\\begin{aligned}\nP(\\theta = 2 \\rvert X = 0) &= \\frac{P(X = 0 \\rvert \\theta = 2)P(\\theta = 2)}\n{\\sum\\limits^3_{j=1}P(X = 0 \\rvert \\theta = j)P(\\theta = j)} \\\\\n&= \\frac{\\frac{1}{10} \\cdot \\frac{5}{10}}{\\frac{1}{10} \\cdot \\frac{5}{10} + 1\n\\cdot \\frac{3}{10} + 1 \\cdot \\frac{2}{10}} \\\\\n&= \\frac{30}{55} = \\frac{6}{11}.\n\\end{aligned}\n\\]\nFinalmente, \\[\nP(\\theta = 3 \\rvert X = 0) = 1 - P(\\theta = 1 \\rvert X = 0) - P(\\theta = 2\n\\rvert X = 0) = \\frac{4}{11}.\n\\]\nPodemos construir nossas tabelas com as distribuições a priori e a posteriori:\n\n\nExemplo 3.2 (Urna com três cores) Considere uma caixa com três cores para \\(5\\) bolas, \\(\\theta_1\\) verdes, \\(\\theta_2\\) brancas \\(5 - \\theta_1 - \\theta_2\\) azuis. Temos que \\(\\theta =\n(\\theta_1, \\theta_2), \\theta \\in \\Theta = \\{(\\mu, \\nu) \\in \\mathbb{N}^2 : \\mu +\n\\nu \\leq 5\\}, \\theta \\sim \\mathrm{Uniforme}(\\Theta)\\). Como \\(\\lvert \\Theta \\rvert\n= 21\\), temos que \\(P(\\theta = (i, j)) = P(\\theta_1 = i, \\theta_2 = j) =\n\\frac{1}{21} \\cdot \\mathbb{1}_{\\Theta}(i,j)\\).\nSuponhamos que colhemos a amostra \\(\\boldsymbol{x} = ((1, 0, 0), (0,0,1))\\), isto é, \\(x_1 = (1,0,0), x_2 = (0,0,1), \\boldsymbol{X}_n = \\boldsymbol{x}\\). Disso, temos que\n\\[\n\\begin{aligned}\nP(\\theta_1 = i, \\theta_2 = j \\rvert \\boldsymbol{X}_n = \\boldsymbol{x}) &\\propto\nP(X_1 = (1,0,0), X_2 = (0,0,1) \\rvert \\theta = (i,j))P(\\theta = (i,j)) \\\\\n&= \\frac{i}{5}\\frac{5-i-j}{4} \\cdot \\frac{1}{21} \\mathbb{1}_{\\Theta}(i,j) \\\\\n\\Rightarrow P(\\theta = (i,j) \\rvert \\boldsymbol{X}_n = \\boldsymbol{x}) & \\propto\ni(5-1-j)\\mathbb{1}_{\\Theta}(i,j)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nO modelo bayesiano\n\n\n\nConsidere o espaço paramétrico \\(\\Theta, \\theta_i \\in \\Theta\\) e uma amostra aleatória \\(\\boldsymbol{X}_n = (X_1, \\dots, X_n)\\) e a amostra observada \\(\\boldsymbol{x}\\). O Teorema de Bayes é aplicado na inferência bayesiana:\n\\[\n\\begin{aligned}\n\\underbrace{P(\\theta = \\theta_i \\rvert \\boldsymbol{X}_n =\n\\boldsymbol{x})}_{\n\\begin{aligned}\n&\\text{probabilidade a posteriori} \\\\\n&\\text{de $\\{\\theta = \\theta_i\\}$ dado $\\boldsymbol{X}_n = \\boldsymbol{x}$}\n\\end{aligned}}\n&= \\frac{P(\\boldsymbol{X}_n =\n\\boldsymbol{x} \\rvert \\theta = \\theta_i) P(\\theta =\n\\theta_i)}{\\underbrace{P(\\boldsymbol{X} =\n\\boldsymbol{x})}_{\\sum\\limits_{j=0}^k P(\\boldsymbol{X} = \\boldsymbol{x} \\rvert\n\\theta = \\theta_i)P(\\theta=\\theta_i)}} \\\\\n&\\propto \\overbrace{P(\\boldsymbol{X} = \\boldsymbol{x} \\rvert \\theta = \\theta_i)}^\n{\n\\begin{aligned}\n\\mathcal{L}_{\n\\boldsymbol{x}}(\\theta_i) \\ &: \\ \\text{verossimilhança gerada por} \\\\\n&\\boldsymbol{x}\\ \\text{no ponto $\\theta_i \\in \\Theta$}\n\\end{aligned}\n} \\cdot\n\\underbrace{P(\\theta = \\theta_i)}_{\\text{probabilidade a priori de $\\{\\theta =\n\\theta_i\\}$}}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Teorema de Bayes</span>"
    ]
  },
  {
    "objectID": "prob-subjetiva.html",
    "href": "prob-subjetiva.html",
    "title": "4  Construção de Probabilidade Subjetiva",
    "section": "",
    "text": "4.1 Opinião do Agente\nConsidere o cenário de um agente contemplando eventos incertos, por exemplo, o resultado de um jogo futuro entre o Inter e o Flamengo (neste caso, o placar do jogo é desconhecido!). Construímos nosso espaço de probabilidades. \\(\\Omega\\) é o conjunto de possíveis placares do jogo, \\(\\Omega = \\{0,1,\\dots,10\\}^2\\). Um agente, como um entusiasta futebolístico dará suas crenças sobre resultados possíveis do jogo.\nSejam \\(A, B\\) eventos. \\(A \\preceq B \\leftrightarrow\\) O agente acredita em \\(B\\) tanto quanto ou mais do que acredita em \\(A\\). No exemplo do futebolista temos, por exemplo, algumas de suas possíveis crenças:\n\\[\n\\begin{aligned}\n\\{(10,10)\\} \\preceq \\{(0,0)\\} \\\\\n\\{(10,10),(9,9)\\} \\preceq \\{(0,0)\\} \\\\\n\\{(6,i) : i \\in \\{0,1,\\dots,10\\}\\} \\preceq \\{(3,i) : i \\in \\{0,1,\\dots,10\\}\\} \\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Construção de Probabilidade Subjetiva</span>"
    ]
  },
  {
    "objectID": "prob-subjetiva.html#opinião-do-agente",
    "href": "prob-subjetiva.html#opinião-do-agente",
    "title": "4  Construção de Probabilidade Subjetiva",
    "section": "",
    "text": "Símbolos de Crença\n\n\n\nOutros símbolos existem para descrever crença. Considere os eventos \\(A\\) e \\(B\\) analisados por um agente. \\[\n\\begin{aligned}\nA \\sim B &\\leftrightarrow \\text{Acredita em $B$ tanto quanto que em $A$} \\\\\nA \\prec B &\\leftrightarrow \\text{Acredita em $B$ mais do que em $A$} \\\\\nA \\preceq B &\\leftrightarrow \\text{Acredita em $B$ tanto quanto ou mais do que\nem $A$}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Construção de Probabilidade Subjetiva</span>"
    ]
  },
  {
    "objectID": "prob-subjetiva.html#relação-de-crença",
    "href": "prob-subjetiva.html#relação-de-crença",
    "title": "4  Construção de Probabilidade Subjetiva",
    "section": "4.2 Relação de Crença",
    "text": "4.2 Relação de Crença\n\n\n\n\n\n\nNotação para Crença\n\n\n\nOutros símbolos existem para descrever crença. Considere os eventos \\(A\\) e \\(B\\) analisados por um agente. \\[\n\\begin{aligned}\nA \\sim B &\\leftrightarrow \\text{Acredita em $B$ igualmente que em $A$} \\\\\nA \\prec B &\\leftrightarrow \\text{Acredita em $B$ mais do que em $A$} \\\\\nA \\preceq B &\\leftrightarrow \\text{Acredita em $B$ tanto quanto ou mais do que\nem $A$}\n\\end{aligned}\n\\]\n\n\nPara \\(A, B\\) eventos, escrevemos \\(A \\preceq B\\) se “\\(B\\) é tão ou mais esperado que \\(A\\)”. Além disso, \\(A \\prec B \\iff A \\preceq B\\) mas \\(B \\not \\preceq A\\) e \\(A \\sim\nB \\iff A \\preceq B\\) e \\(B \\preceq A\\).\nTemos como objetivo especificar condições sobre a relação de crença, \\(\\preceq\\), de modo a assegurar que tal relação admita uma representação numérica por uma probabilidade no seguinte sentido: Para \\(A, B\\) eventos,\n\\[\nA \\preceq B \\iff P(A) \\leq P(B).\n\\]\nAlgumas condições sobre \\(\\preceq\\):\n\nS1 : Para todo \\(A, B\\), \\(A \\preceq B\\) ou \\(B \\preceq A\\).\nS2 : Sejam \\(A_1, A_2, B_1, B_2\\) eventos tais que \\(A_1 \\cap A_2 = B_1 \\cap B_2 =\n\\emptyset\\). Então \\(A_i \\preceq B_i, i = 1, 2 \\Rightarrow A_1 \\cup A_2 \\preceq B_1 \\cup\nB_2\\). Adicionalmente, \\(A_i \\prec B_i\\) para algum \\(i \\Rightarrow A_1 \\cup A_2\n\\prec B_1 \\cup B_2\\).\nS3 : Para todo evento \\(A, A \\preceq \\Omega\\). Além disso, \\(\\emptyset \\prec\n\\Omega\\).\nEstabelecemos S4, S5 e S6 que não abordaremos no momento.\n\nSeguem consequências dessas condições:\n\nSejam \\(A, B, C\\) eventos tais que \\(A \\cap B = B \\cap C = \\emptyset\\). Então, \\(A\n\\preceq B \\iff A \\cup C \\preceq B \\cup C\\).\n\nVerificamos a ida (\\(\\Rightarrow\\)) tomando S2: \\(A_1 = A, B_1 = B, A_2 = B_2 = C\\). Como \\(A \\preceq B\\) e \\(C \\preceq C\\) e \\(A \\cap C = B \\cap C = \\emptyset\\), segue que \\(A \\cup C \\preceq B \\cup C\\).\n\nExercício 4.1 Verifique a volta (\\(\\Leftarrow\\)). Dica: utilize a contra-positiva \\[\nB \\prec A \\Rightarrow B \\cup C \\prec A \\cup C.\n\\]\n\n\n\\(A \\preceq B \\iff B^C \\preceq A^C\\).\n\nVerificamos (\\(\\Rightarrow\\)): \\(A \\preceq B \\Rightarrow (A \\cap B) \\cup (A \\cap B^C) \\preceq (B \\cap A)\n\\cup (B \\cap A^C)\\). Tome \\(C' = A \\cap B\\). Pelo resultado 1., \\(A \\cap B^C \\preceq\nB \\cap A^C \\Rightarrow (A \\cap B^C) \\cup (A^C \\cap B^C) \\preceq B (\\cap A^C)\n\\cup (A^C \\cap B^C)\\), finalmente, \\(B^C \\preceq A^C\\). A volta é análoga.\n\nSejam \\(A, B, C\\) eventos. \\(A \\preceq B\\) e \\(B \\preceq C \\Rightarrow\\).\n\nVerificando: \\[\n\\begin{aligned}\nA \\preceq B &\\Rightarrow (A \\cap B) \\cup (A \\cap B^C) \\preceq (B \\cap A) \\cup (B\n\\cap A^C) \\\\\n&\\stackrel{1.}{\\Rightarrow} A \\cap B^C \\preceq B \\cap A^C \\ \\ (\\mathrm{I}) \\\\ \\\\\nB \\preceq C &\\Rightarrow (B \\cap C) \\cup (B \\cap C^C) \\preceq (C \\cap B) \\cup (C\n\\cap B^C) \\\\\n&\\stackrel{1.}{\\Rightarrow} B \\cap C^C \\preceq C \\cap B^C \\ \\ (\\mathrm{II}) \\\\\n\\\\\n&\\stackrel{S2}{\\Rightarrow} (A \\cap B^C) \\cup (B\\cap C^C) \\preceq (A^C \\cap B)\n\\cup (B^C \\cap C) \\\\\n&\\Rightarrow (A \\cap B^C \\cap C) \\cup (A \\cap B^C  \\cap C^C) \\cup (A \\cap B \\cap\nC^C) \\cup (A^C \\cap B \\cap C^C) \\\\ &\\preceq (A^C \\cap B \\cap C) \\cup (A^C \\cap B\n\\cap C^C) \\cup (A \\cap B^C \\cap C) \\cup (A^C \\cap B^C \\cap C) \\\\ \\\\\n&\\stackrel{1.}{\\Rightarrow} (A \\cap B^C \\cap C) \\cup (A \\cap B^C \\cap C^C) \\cup\n(A \\cap B \\cap C^C) \\\\ &\\preceq (A^C \\cap B \\cap C) \\cup (A \\cap B^C \\cap C) \\cup\n(A^C \\cap B^C \\cap C) \\\\ \\\\\n&\\stackrel{1.}{\\Rightarrow} (A \\cap B^C \\cap C) \\cup (A \\cap B^C \\cap C^C) \\cup\n(A \\cap B \\cap C^C) \\cup (A \\cap B \\cap C) \\\\ &\\preceq (A^C \\cap B \\cap C) \\cup (A \\cap B^C \\cap C) \\cup\n(A^C \\cap B^C \\cap C) \\cup (A \\cap B \\cap C) \\\\\n&\\Rightarrow A \\preceq C.\n\\end{aligned}\n\\]\n\nDe S3, para todo \\(A\\), \\(\\emptyset \\preceq A\\).\n\nCom efeito, \\(A^C \\preceq \\Omega\\). Pela consequência 2., \\(\\Omega^C \\preceq\n(A^C)^C\\), isto é, \\(\\emptyset \\preceq A\\). Consequentemente, para todo \\(A\\),\n\\[\n\\emptyset \\preceq A \\preceq \\Omega.\n\\]\n\nSejam eventos \\(A,B\\), \\(A \\subseteq B \\Rightarrow A \\preceq B\\).\n\nVerificamos: \\(B = A \\cup (B \\cap A^C)\\). Vimos que \\(A \\subseteq A \\ (I)\\). Além disso, \\(\\emptyset \\subseteq B \\cap A^C \\\n(II)\\). Então, pelo S2, \\(A \\cap \\emptyset \\preceq A \\cup (B \\cap A^C) \\Rightarrow\nA \\preceq B\\).\n\nTeorema 4.1 Seja \\(\\preceq\\) uma relação de crença satisfazendo as suposições S1 até S6. Então, existe e é única probabilidade \\(P : \\{\\mathrm{Eventos}\\} \\rightarrow\n[0,1]\\) tal que para quaisquer eventos \\(A\\) e \\(B\\), \\[\nA \\preceq B \\iff P(A) \\leq P(B).\n\\]",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Construção de Probabilidade Subjetiva</span>"
    ]
  }
]