[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anotações de Inferência Bayesiana",
    "section": "",
    "text": "1 Inferência Bayesiana",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Inferência Bayesiana</span>"
    ]
  },
  {
    "objectID": "pensamento-bayesiano.html",
    "href": "pensamento-bayesiano.html",
    "title": "2  O Pensamento Bayesiano",
    "section": "",
    "text": "2.1 Inferência\nPodemos dizer que inferência é qualquer processo racional de redução de incerteza. Se não há incerteza, não há necessidade da estatística.\nNote que, mesmo após esse processo, a incerteza sobre a composição da caixa contínua. Contudo, com a nova informação, ocorre uma atualização da incerteza.\nA inferência estatística é, portanto, o processo de redução de incertezas a partir de dados e métodos estatísticos. Neste contexto, a Inferência Bayesiana é a inferência estatística baseada na perspectiva subjetiva de probabilidade.",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O Pensamento Bayesiano</span>"
    ]
  },
  {
    "objectID": "pensamento-bayesiano.html#inferência",
    "href": "pensamento-bayesiano.html#inferência",
    "title": "2  O Pensamento Bayesiano",
    "section": "",
    "text": "Exemplo 2.1 (Bolas na caixa) Considere uma caixa com 5 bolas, algumas verdes (\\(v\\)), e o restante brancas (\\(b\\)). Suponha que amostramos duas bolas, sem reposição. Isto é, retiramos duas bolas desta caixa, e que ambas as bolas retiradas foram verdes. A partir deste experimento, podemos inferir que \\(v \\geq 2\\), ou seja, ao menos duas bolas da caixa são verdes. Caso retirássemos uma branca e uma verde, poderíamos naturalmente inferir que ao menos uma bola da caixa é branca, e ao menos uma é verde.\n\n\n\n\n\n\n\n\n\nProbabilidade\n\n\n\nO objeto probabilidade pode ser estudada de diversas perspectivas. Estamos acostumados com o cálculo de probabilidade, em que desenvolvemos teoremas e outros resultados a partir dos axiomas de Kolmogorov.\n\\[\n\\begin{aligned}\nP(A) \\geq 0, \\forall A \\in \\mathcal{F} \\\\\nP(\\Omega) = 1 \\\\\nP\\left(\\bigcup\\limits_{n=1}^\\infty A_n\\right) = \\sum^\\infty_{n=1} A_n\n\\end{aligned}\n\\]\nPor outro lado, há pessoas que se ocupam em entender a probabilidade do ponto de vista da Teoria da Medida, preocupando-se em estudar a probabilidade como uma função de medida.\nAinda assim, existe uma visão mais “filosófica” do estudo da probabilidade, que se preocupa na interpretação da probabilidade. Uma interpretação clássica é da probabilidade como medida de frequência, enquanto a interpretação subjetiva da probabilidade, utilizada na inferência Bayesiana, toma a probabilidade como uma medida de incerteza.\nPor exemplo, num lançamento de moedas, a teoria frequentista afirma que, ao lançar uma moeda honesta um número grande de vezes, esperaria que 50% dos lançamentos resultasse em cara. Por outro lado, na teoria subjetiva da probabilidade, alguém que acredita que a moeda é honesta diria, antes do primeiro lançamento desta, que não há razão de acreditar que a moeda vá provavelmente cair cara, tão quanto acreditar que mais provavelmente cairá coroa. Nesta visão, a probabilidade é um número que mede, quantifica ou representa a incerteza do observador sobre um fenômeno.\nEsta discussão sobre o significado de probabilidade é denominado Teoria da Probabilidade.\n\n\n\n2.1.1 Cenários de Incerteza\nAlém do exemplo da caixa com bolas que discutimos anteriormente, podemos pensar em outros cenários de incerteza:\n\nExemplo 2.2 (Erupções na pele) Considere um indivíduo com erupções na pele. Este tipo de erupções pode ser causado por doenças diversas, dentre elas, uma é considerada grave, provocando incerteza no indivíduo sobre sua saúde. Sabendo disto, o indivíduo vai ao hospital em busca de informações sobre sua condição. A partir de experimentos, como exames médicos, a incerteza do indivíduo sobre a doença e sua saúde é reduzida pelas informações obtidas.\n\n\nExemplo 2.3 (Avião perdido) Em outro exemplo, considere que um avião caiu num corpo d’água e sua localização exata é desconhecida. Conforme descobrimos informações do avião, como partes encontradas no mar, consideração de oceanográficos e climáticos, análise técnica do modelo do avião, resultados de buscas passadas e outras informações, podemos gradativamente reduzir a região de queda do avião, até encontrarmos o ponto exato ou muito próximo desta queda.\nEste exemplo é mais real do que pensa! Técnicas bayesianas foram utilizadas na busca pelo infame voo 447\n\nNum último exemplo, suponha que estamos interessados em estudar a eficácia de um medicamento na redução da taxa do colesterol. A incerteza jaz exatamente na eficácia do medicamento, isto é, quanto a taxa de colesterol é reduziada após uso do medicamento. Para esta análise, coletaríamos amostras aleatórias simples. Suponha, por hipótese, que o colesterol dos pacientes em estudo amostrados (\\(\\boldsymbol{X}_n = (X_1, \\dots, X_n)\\) seguem uma distribuição Normal, com variância 16 e média desconhecida que queremos inferir:\n\\[\nX \\sim (?,16)\n\\]\nNo modelo clássico, usaríamos de métodos como o uso do estimador não viesado de variância uniformemente mínima. Por outro lado, o bayesiano começaria com uma incerteza antes da coleta da amostra e, com a informação obtida, atualizaria sua incerteza em uma nova distribuição: \\[\n\\mathcal{D} \\stackrel{\\boldsymbol{X}_n}{\\rightarrow} \\mathcal{D} \\lvert x_1,\n\\dots, x_n\n\\]\nEsta ferramenta de atualização de informação é o cerne da abordagem Bayesiana, e será extensivamente estudada pelo curso.",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>O Pensamento Bayesiano</span>"
    ]
  },
  {
    "objectID": "teorema-de-bayes.html",
    "href": "teorema-de-bayes.html",
    "title": "3  Teorema de Bayes",
    "section": "",
    "text": "Da probabilidade, conhecemos o enunciado do Teorema de Bayes.\n\nTeorema 3.1 (Teorema de Bayes) Sejam \\(A_1, A_2, ... A_n \\in \\mathcal{A}\\) eventos que particionam \\(\\Omega\\), isto é, \\(A_i \\cap A_j = \\emptyset, \\bigcup_{i=1}^n A_i = \\Omega,\\) e \\(D \\in\n\\mathcal{A}\\) um evento tal que \\(P(D) &gt; 0\\). Então, para \\(i = 1, 2, \\dots, n\\),\n\\[\nP(A_i \\rvert D) = \\frac{P(D \\rvert A_i) P(A_i)}{\\sum\\limits^{n}_{j = 1} P(D \\rvert\nA_j)P(A_j)}\n\\]\n\n\n\n\n\n\n\nDetalhes importantes\n\n\n\nNote que \\(P(X=x \\rvert \\theta = \\theta_i)\\) é a função de verossimilhança. Note também que o denominador, \\(\\sum\\limits^{n}_{j = 1} P(D \\rvert A_j)P(A_j)\\), não depende de \\(i\\)!!!\n\n\n\nNote que, para o Exemplo 2.1,  \n\\[\n\\begin{aligned}\nE(\\theta) &= \\frac{5}{2} \\\\\nE(\\theta \\rvert X_1 = 1, X_2 = 0)  &= 1 \\cdot \\frac{2}{10} + 2 \\cdot {3}{10} + 3\n\\cdot \\frac {3}{10} + 4 \\cdot \\frac{2}{10} = \\frac{5}{2} \\\\\n\\mathrm{Var}(\\theta \\rvert X_1 = 1, X_2 = 0) &= E(\\theta^2 \\rvert X_1 = 1, X_2=  0) - E(\\theta \\rvert X_1 = 1, X_2\n= 0)^2 \\\\\n&= 1^2 \\cdot \\frac{2}{10} + 2^2 \\cdot \\frac{3}{10} + 3^2\n\\cdot \\frac{3}{10} + 4^2 \\cdot \\frac{2}{10} - \\frac{25}{4} = \\frac{21}{20}\n\\end{aligned}\n\\]\n\nExercício 3.1 Refaça o exercício (encontre a distribuição a posteriori de \\(\\theta\\)) dado a amostra \\(X_1 = 1, X_2 = 1\\)\n\n\nSolução 3.1. Pelo Teorema 3.1,\n\\[\n\\frac{\\frac{i}{5} \\cdot \\frac{i-1}{4} \\cdot \\frac{1}{6}}{\\sum\\limits^5_{j=0}\n\\frac{j}{5} \\cdot \\frac{j-1}{4} \\cdot \\frac{1}{6}} = \\frac{i(i-1)}\n{\\sum\\limits^5_{j=0}j(j-1)} = \\frac{i(i-1)}{40}\n\\] \nUm outro caminho para resolução envolve o uso de proporcionalidade. Podemos observar que, pelo Teorema 3.1, \\[\n\\begin{aligned}\nP(\\theta = i \\rvert \\boldsymbol{X} = (1,1)) &\\propto\nP(\\boldsymbol{X} = (1,1) \\rvert \\theta = i)P(\\theta = i) = \\frac{i}{5} \\cdot\n\\frac{i-1}{4} \\cdot \\frac{1}{6} \\mathbb{1}_{\\Theta}(i) \\\\\n&= \\frac{i(i-1)}{120}\n\\mathbb{1}_{\\Theta}(i) \\propto i(i-1) \\mathbb{1}_{\\Theta}(i)\n\\end{aligned}\n\\]\nou seja, podemos simplesmente nos preocupar com o numerador e dividir (“normalizar”) pela soma dos termos: \n\n\nExemplo 3.1 (Informação sobre a soma) Ainda no cenário do Exemplo 2.1, considere a função \\(T(\\boldsymbol{X}) = X_1 +\nX_2\\) do número de bolas verdes na amostra. Você é informado que \\(T(X_ = 1\\). Vamos determinar a distribuição a posteriori de \\(\\theta\\) dado que \\(X_1 + X_2 = 1\\).\n\\[\nP(\\theta = i \\rvert X_1 + X_2 = 1) = \\frac{P(X_1 + X_2 = 1 \\rvert \\theta =\ni)P(\\theta=i)}{\\sum\\limits^5_{j=0}P(X_1 + X_2 = 1 \\rvert \\theta = j)P(\\theta=j)}\n\\] usando da proporcionalidade, \\[\n\\begin{aligned}\nP(\\theta = i \\rvert X_1 + X_2 = 1) &\\propto P(X_1 + X_2 \\rvert \\theta =\ni)P(\\theta = i)\\\\\n&= \\sum\\limits_{(x_1,x_2) \\in \\mathfrak{X} : x_1 + x_2 = 1}\nP(X_1 = x_1, X_2 = x_2 \\rvert \\theta = i)P(\\theta = i) \\\\\n&= [P(X_1 = 1, X_2 = 0 \\rvert \\theta = i) + P(X_1 = 0, X_2 = 1 \\rvert \\theta = i)]P(\\theta = i) \\\\\n&= \\left[\\frac{i}{5} \\cdot \\frac{(5-i)}{4} + \\frac{5-i}{5} \\cdot \\frac{i}{4}\\right] \\cdot \\frac{1}{6} \\mathbb{1}_\\Theta(i) \\\\\n&= \\frac{i(5-i)}{10} \\cdot \\frac{1}{6} \\mathbb{1}_\\Theta(i) \\propto i(5-1)\n\\mathbb{1}_\\Theta(i)\n\\end{aligned}\n\\]\nNote que isso nos concede a mesma quantidade de informação para a inferência que a amostra completa, ou seja, obtemos a mesma a posteriori utilizando esta estatística. Isto ocorre uma vez que a função \\(T(\\boldsymbol{X})\\) é uma estatística suficiente no sentido bayesiano. Mais adiante, discutiremos isso com mais detalhes.",
    "crumbs": [
      "Inferência Bayesiana",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Teorema de Bayes</span>"
    ]
  }
]